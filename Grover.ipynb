{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baec2f57",
   "metadata": {},
   "source": [
    "# STAT 5243 Final Project -- GROVER Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa3e16",
   "metadata": {},
   "source": [
    "## Install & import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185bca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pubchempy in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (1.0.4)\n",
      "Requirement already satisfied: pandas in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numpy in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: transformers in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: torch in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: rdkit in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (2025.9.2)\n",
      "Requirement already satisfied: numpy in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from rdkit) (2.0.2)\n",
      "Requirement already satisfied: Pillow in /Users/alisahe/Desktop/大学/Columbia/FA25/STAT 5243/Final Project/Coding/ChemBERTa/.venv/lib/python3.9/site-packages (from rdkit) (11.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pubchempy\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install transformers \n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "id": "50842987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:18:20.394336Z",
     "start_time": "2025-12-18T09:18:20.342111Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pubchempy as pcp\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle \n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from argparse import Namespace\n",
    "import argparse\n",
    "from rdkit import Chem"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "b7f03c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:18:20.762089Z",
     "start_time": "2025-12-18T09:18:20.716248Z"
    }
   },
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ec2f5dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:18:21.196558Z",
     "start_time": "2025-12-18T09:18:21.148328Z"
    }
   },
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "e6710915",
   "metadata": {},
   "source": [
    "## 1. STRINGs Retrieval"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:18:22.665574Z",
     "start_time": "2025-12-18T09:18:22.613154Z"
    }
   },
   "cell_type": "code",
   "source": "base_path = \"/root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/\"",
   "id": "de2dc5bdd4a90c7e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "21f2d8ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:18:25.464999Z",
     "start_time": "2025-12-18T09:18:23.111571Z"
    }
   },
   "source": [
    "file_path = base_path+'bio-decagon-combo.csv'\n",
    "\n",
    "print(f\"Loading data from {file_path} to extract unique drug IDs...\")\n",
    "df_combo = pd.read_csv(file_path)\n",
    "\n",
    "unique_drug_a = df_combo['STITCH 1'].unique()\n",
    "unique_drug_b = df_combo['STITCH 2'].unique()\n",
    "\n",
    "all_unique_drug_ids = pd.unique(np.concatenate((unique_drug_a, unique_drug_b)))\n",
    "\n",
    "print(f\"Total unique drug IDs found: {len(all_unique_drug_ids)}\")\n",
    "print(\"Example IDs:\", all_unique_drug_ids[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/bio-decagon-combo.csv to extract unique drug IDs...\n",
      "Total unique drug IDs found: 645\n",
      "Example IDs: ['CID000002173' 'CID000005206' 'CID000003929' 'CID000001302'\n",
      " 'CID000005267']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71248ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fetch SMILES for 645 drugs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dz/ykmlvvnx7gg2p91fhyn9gn600000gn/T/ipykernel_1274/1150505511.py:26: PubChemPyDeprecationWarning: canonical_smiles is deprecated: Use connectivity_smiles instead\n",
      "  if compound and compound.canonical_smiles:\n",
      "/var/folders/dz/ykmlvvnx7gg2p91fhyn9gn600000gn/T/ipykernel_1274/1150505511.py:27: PubChemPyDeprecationWarning: canonical_smiles is deprecated: Use connectivity_smiles instead\n",
      "  smiles = compound.canonical_smiles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 20 / 645 CIDs...\n",
      "  Processed 40 / 645 CIDs...\n",
      "  Processed 60 / 645 CIDs...\n",
      "  Processed 80 / 645 CIDs...\n",
      "  Processed 100 / 645 CIDs...\n",
      "  Processed 120 / 645 CIDs...\n",
      "  Processed 140 / 645 CIDs...\n",
      "  Processed 160 / 645 CIDs...\n",
      "  Processed 180 / 645 CIDs...\n",
      "  Processed 200 / 645 CIDs...\n",
      "  Processed 220 / 645 CIDs...\n",
      "  Processed 240 / 645 CIDs...\n",
      "  Processed 260 / 645 CIDs...\n",
      "  Processed 280 / 645 CIDs...\n",
      "  Processed 300 / 645 CIDs...\n",
      "  Processed 320 / 645 CIDs...\n",
      "  Processed 340 / 645 CIDs...\n",
      "  Processed 360 / 645 CIDs...\n",
      "  Processed 380 / 645 CIDs...\n",
      "  Processed 400 / 645 CIDs...\n",
      "  Processed 420 / 645 CIDs...\n",
      "  Processed 440 / 645 CIDs...\n",
      "  Processed 460 / 645 CIDs...\n",
      "  Processed 480 / 645 CIDs...\n",
      "  Processed 500 / 645 CIDs...\n",
      "  Processed 520 / 645 CIDs...\n",
      "  Processed 540 / 645 CIDs...\n",
      "  Processed 560 / 645 CIDs...\n",
      "  Processed 580 / 645 CIDs...\n",
      "  Processed 600 / 645 CIDs...\n",
      "  Processed 620 / 645 CIDs...\n",
      "  Processed 640 / 645 CIDs...\n",
      "...Fetching complete. Processed 645 CIDs.\n",
      "\n",
      "--- SMILES Retrieval Results ---\n",
      "     cid_string                                             smiles status\n",
      "0  CID000002173  CC1(C(N2C(S1)C(C2=O)NC(=O)C(C3=CC=CC=C3)N)C(=O...  Found\n",
      "1  CID000005206                           C(OC(C(F)(F)F)C(F)(F)F)F  Found\n",
      "2  CID000003929      CC(=O)NCC1CN(C(=O)O1)C2=CC(=C(C=C2)N3CCOCC3)F  Found\n",
      "3  CID000001302                CC(C1=CC2=C(C=C1)C=C(C=C2)OC)C(=O)O  Found\n",
      "4  CID000005267  CC(=O)SC1CC2=CC(=O)CCC2(C3C1C4CCC5(C4(CC3)C)CC...  Found\n"
     ]
    }
   ],
   "source": [
    "def get_smiles_from_cids(cid_list):\n",
    "    drug_data = []\n",
    "    total_cids = len(cid_list)\n",
    "    \n",
    "    print(f\"Starting to fetch SMILES for {total_cids} drugs...\")\n",
    "    \n",
    "    for i, cid_string in enumerate(cid_list):\n",
    "        smiles = None\n",
    "        status = 'Error'\n",
    "        cid_number = None\n",
    "\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"  Processed {i + 1} / {total_cids} CIDs...\")\n",
    "\n",
    "        try:\n",
    "            # clean the strings (e.g., \"CID000002173\" -> 2173)\n",
    "            if isinstance(cid_string, str) and cid_string.startswith('CID'):\n",
    "                cid_number = int(cid_string.replace('CID', ''))\n",
    "            else:\n",
    "                cid_number = int(cid_string)\n",
    "\n",
    "            # retrieve from PubChem\n",
    "            compound = pcp.Compound.from_cid(cid_number)\n",
    "            \n",
    "            # get the smiles strings\n",
    "            if compound and compound.canonical_smiles:\n",
    "                smiles = compound.canonical_smiles\n",
    "                status = 'Found'\n",
    "            else:\n",
    "                status = 'Not Found'\n",
    "\n",
    "            drug_data.append({\n",
    "                'cid_string': cid_string,\n",
    "                'smiles': smiles,\n",
    "                'status': status\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # handel any not found STICH IDs\n",
    "            drug_data.append({\n",
    "                'cid_string': cid_string,\n",
    "                'smiles': None,\n",
    "                'status': f'Error: {e}'\n",
    "            })\n",
    "        \n",
    "        # stop periodically for api requests\n",
    "        time.sleep(0.21) \n",
    "\n",
    "    print(f\"...Fetching complete. Processed {total_cids} CIDs.\")\n",
    "    \n",
    "    df_results = pd.DataFrame(drug_data)\n",
    "    return df_results\n",
    "\n",
    "smiles_df = get_smiles_from_cids(all_unique_drug_ids)\n",
    "\n",
    "print(\"\\n--- SMILES Retrieval Results ---\")\n",
    "print(smiles_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bd03c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_df.to_csv(base_path + 'smiles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c5bb69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid_string</th>\n",
       "      <th>smiles</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cid_string, smiles, status]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_df[smiles_df['status'] != 'Found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47c50272",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_df.to_csv(base_path + 'drug_smiles_mapping_full.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirms uniqueness of the strings\n",
    "def verify_structural_uniqueness(csv_file):\n",
    "    print(f\"Loading data from {csv_file}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {csv_file} not found.\")\n",
    "        return\n",
    "\n",
    "    if 'smiles' not in df.columns:\n",
    "        print(\"Error: 'smiles' column not found in the CSV.\")\n",
    "        return\n",
    "\n",
    "    # Filter out any rows where smiles might be missing just in case\n",
    "    initial_count = len(df)\n",
    "    df = df.dropna(subset=['smiles'])\n",
    "    if len(df) < initial_count:\n",
    "        print(f\"Warning: Dropped {initial_count - len(df)} rows with missing SMILES.\")\n",
    "\n",
    "    print(f\"Processing {len(df)} SMILES strings...\")\n",
    "    \n",
    "    inchi_keys = []\n",
    "    invalid_smiles_count = 0\n",
    "\n",
    "    for idx, smiles in enumerate(df['smiles']):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            # Generate InChIKey\n",
    "            inchi_key = Chem.MolToInchiKey(mol)\n",
    "            inchi_keys.append(inchi_key)\n",
    "        else:\n",
    "            print(f\"Warning: Invalid SMILES at row {idx}: {smiles}\")\n",
    "            invalid_smiles_count += 1\n",
    "            inchi_keys.append(None)\n",
    "\n",
    "    df['InChIKey'] = inchi_keys\n",
    "    \n",
    "    # Remove rows where InChIKey could not be generated\n",
    "    df_valid = df.dropna(subset=['InChIKey'])\n",
    "    \n",
    "    total_valid = len(df_valid)\n",
    "    unique_inchikeys = df_valid['InChIKey'].nunique()\n",
    "\n",
    "    print(\"\\n--- Structural Uniqueness Verification Results ---\")\n",
    "    print(f\"Total valid SMILES processed: {total_valid}\")\n",
    "    print(f\"Total unique InChIKeys: {unique_inchikeys}\")\n",
    "\n",
    "    if total_valid == unique_inchikeys:\n",
    "        print(\"\\n✅ SUCCESS: All processed SMILES strings produced unique full InChIKeys.\")\n",
    "        print(\"This confirms that no duplicate or structurally redundant compounds are present in the dataset.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ ISSUE DETECTED: Found {total_valid - unique_inchikeys} duplicate entries based on InChIKeys.\")\n",
    "        duplicates = df_valid[df_valid.duplicated(subset=['InChIKey'], keep=False)]\n",
    "        print(\"Duplicate entries:\")\n",
    "        print(duplicates[['cid_string', 'smiles', 'InChIKey']].sort_values(by='InChIKey'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c310bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from drug_smiles_mapping_full.csv...\n",
      "Processing 645 SMILES strings...\n",
      "\n",
      "--- Structural Uniqueness Verification Results ---\n",
      "Total valid SMILES processed: 645\n",
      "Total unique InChIKeys: 645\n",
      "\n",
      "✅ SUCCESS: All processed SMILES strings produced unique full InChIKeys.\n",
      "This confirms that no duplicate or structurally redundant compounds are present in the dataset.\n"
     ]
    }
   ],
   "source": [
    "csv_path = base_path+'drug_smiles_mapping_full.csv'\n",
    "verify_structural_uniqueness(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f06351",
   "metadata": {},
   "source": [
    "## 2. GROVER Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7058dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.serialization.add_safe_globals([argparse.Namespace])"
   ]
  },
  {
   "cell_type": "code",
   "id": "972575be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:31:07.439277Z",
     "start_time": "2025-12-18T09:31:03.035001Z"
    }
   },
   "source": [
    "GROVER_REPO_DIR = os.path.abspath(os.path.join(base_path, \"grover\"))\n",
    "if GROVER_REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, GROVER_REPO_DIR)\n",
    "    print(f\"Added {GROVER_REPO_DIR} to sys.path\")\n",
    "\n",
    "try:\n",
    "    from grover.util.utils import load_args, load_checkpoint\n",
    "    from grover.model.models import GroverFpGeneration\n",
    "    from grover.data.moldataset import MoleculeDataset, MoleculeDatapoint\n",
    "    from grover.data.molgraph import MolCollator\n",
    "\n",
    "    from rdkit import Chem\n",
    "    from rdkit import RDLogger\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"GROVER/RDKit import failed: {e}\\n\"\n",
    "        f\"Ensure ./grover exists and RDKit is installed.\"\n",
    "    )\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.warning\")\n",
    "\n",
    "SMILES_FILE = base_path + \"drug_smiles_mapping_full.csv\"\n",
    "OUTPUT_FILE = base_path + \"grover_drug_fingerprints_large.pkl\"\n",
    "GROVER_MODEL_PATH = base_path + \"grover/grover/model/grover_large.pt\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Loading SMILES data from {SMILES_FILE}...\")\n",
    "df_smiles = pd.read_csv(SMILES_FILE)\n",
    "df_found_smiles = df_smiles[df_smiles[\"status\"] == \"Found\"].copy()\n",
    "\n",
    "cid_list = df_found_smiles[\"cid_string\"].tolist()\n",
    "smiles_list = df_found_smiles[\"smiles\"].tolist()\n",
    "print(f\"Found {len(smiles_list)} SMILES strings for processing.\")\n",
    "\n",
    "# PyTorch >=2.6 uses weights_only=True by default in torch.load; allowlist argparse.Namespace\n",
    "# so GROVER checkpoints (which store args as a Namespace) can be loaded.\n",
    "if hasattr(torch, \"serialization\") and hasattr(torch.serialization, \"add_safe_globals\"):\n",
    "    torch.serialization.add_safe_globals([argparse.Namespace])\n",
    "\n",
    "print(f\"\\nLoading GROVER model args from {GROVER_MODEL_PATH}...\")\n",
    "model_args = load_args(GROVER_MODEL_PATH)\n",
    "\n",
    "if not hasattr(model_args, \"parser_name\"):\n",
    "    model_args.parser_name = \"fingerprint\"\n",
    "if not hasattr(model_args, \"output_size\"):\n",
    "    model_args.output_size = 1\n",
    "if not hasattr(model_args, \"cuda\"):\n",
    "    model_args.cuda = (DEVICE.type == \"cuda\")\n",
    "if not hasattr(model_args, \"dropout\"):\n",
    "    model_args.dropout = 0.1\n",
    "if not hasattr(model_args, \"features_generator\"):\n",
    "    model_args.features_generator = []\n",
    "if not hasattr(model_args, \"no_cache\"):\n",
    "    model_args.no_cache = False\n",
    "if not hasattr(model_args, \"embedding_output_type\"):\n",
    "    model_args.embedding_output_type = \"both\"\n",
    "    print(\"⚠️ Added missing attribute: embedding_output_type='both'\")\n",
    "if not hasattr(model_args, \"fingerprint_source\"):\n",
    "    model_args.fingerprint_source = \"atom\"\n",
    "    print(\"⚠️ Added missing attribute: fingerprint_source='atom'\")\n",
    "\n",
    "print(f\"\\nLoading GROVER checkpoint from {GROVER_MODEL_PATH}...\")\n",
    "model = load_checkpoint(GROVER_MODEL_PATH, current_args=model_args)\n",
    "print(f\"Loaded GROVER checkpoint (model) successfully.\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def make_datapoint(smiles: str, args: Namespace) -> MoleculeDatapoint:\n",
    "    num_tasks = getattr(args, \"num_tasks\", getattr(args, \"output_size\", 1))\n",
    "    line_with_placeholders = [smiles] + [\"\"] * int(num_tasks)\n",
    "    return MoleculeDatapoint(line=line_with_placeholders, args=args, features=None)\n",
    "\n",
    "data_points = []\n",
    "valid_cids = []\n",
    "\n",
    "for cid, smiles in zip(cid_list, smiles_list):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        continue\n",
    "    try:\n",
    "        dp = make_datapoint(smiles, model_args)\n",
    "        data_points.append(dp)\n",
    "        valid_cids.append(cid)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: Failed to create Datapoint for {cid}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Valid SMILES: {len(valid_cids)}\")\n",
    "\n",
    "try:\n",
    "    dataset = MoleculeDataset(data_points)\n",
    "except Exception:\n",
    "    class SimpleListDataset(MoleculeDataset):\n",
    "        def __init__(self, data):\n",
    "            self._data = data\n",
    "            self.features_size = 0\n",
    "        def __len__(self):\n",
    "            return len(self._data)\n",
    "        def __getitem__(self, idx):\n",
    "            return self._data[idx]\n",
    "    dataset = SimpleListDataset(data_points)\n",
    "\n",
    "shared_dict = {}\n",
    "collator = MolCollator(shared_dict=shared_dict, args=model_args)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "fp_map = {}\n",
    "print(\"\\nGenerating GROVER fingerprints (molecule-level)...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    offset = 0\n",
    "    inferred_dim = None\n",
    "\n",
    "    for batch_idx, batch_tuple in enumerate(data_loader):\n",
    "        smiles_batch, mol_batch, features_batch, mask, targets = batch_tuple\n",
    "        \n",
    "        # Forward pass\n",
    "        fp = model(mol_batch, features_batch)\n",
    "\n",
    "        if fp.ndim != 2:\n",
    "            raise RuntimeError(f\"Unexpected fp.ndim={fp.ndim}, expected 2.\")\n",
    "\n",
    "        fp_cpu = fp.detach().cpu().numpy()\n",
    "\n",
    "        if inferred_dim is None:\n",
    "            inferred_dim = fp_cpu.shape[1]\n",
    "            print(f\"Inferred fingerprint dim = {inferred_dim}\")\n",
    "\n",
    "        current_cids = valid_cids[offset: offset + fp_cpu.shape[0]]\n",
    "        offset += fp_cpu.shape[0]\n",
    "\n",
    "        for cid, vec in zip(current_cids, fp_cpu):\n",
    "            fp_map[cid] = vec\n",
    "\n",
    "print(\"...GROVER fingerprint generation complete.\")\n",
    "print(f\"Generated {len(fp_map)} fingerprints.\")\n",
    "\n",
    "# ====== 8) Save ======\n",
    "print(f\"\\nSaving GROVER fingerprints to: {OUTPUT_FILE}\")\n",
    "with open(OUTPUT_FILE, \"wb\") as f:\n",
    "    pickle.dump(fp_map, f)\n",
    "\n",
    "print(f\"Success! Saved to '{OUTPUT_FILE}'.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SMILES data from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/drug_smiles_mapping_full.csv...\n",
      "Found 645 SMILES strings for processing.\n",
      "\n",
      "Loading GROVER model args from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/grover/grover/model/grover_large.pt...\n",
      "⚠️ Added missing attribute: fingerprint_source='atom'\n",
      "\n",
      "Loading GROVER checkpoint from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/grover/grover/model/grover_large.pt...\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
      "Loaded GROVER checkpoint (model) successfully.\n",
      "Valid SMILES: 645\n",
      "\n",
      "Generating GROVER fingerprints (molecule-level)...\n",
      "Inferred fingerprint dim = 2400\n",
      "...GROVER fingerprint generation complete.\n",
      "Generated 645 fingerprints.\n",
      "\n",
      "Saving GROVER fingerprints to: /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/grover_drug_fingerprints_large.pkl\n",
      "Success! Saved to '/root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/grover_drug_fingerprints_large.pkl'.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "da30ede4",
   "metadata": {},
   "source": [
    "## 3. Contrastive Loss"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:31:15.589661Z",
     "start_time": "2025-12-18T09:31:15.446224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(base_path+'grover_drug_fingerprints_large.pkl', 'rb') as f:\n",
    "    drug_embeddings_map = pickle.load(f)"
   ],
   "id": "dabdd3fb2408615d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "48b52caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:31:16.715638Z",
     "start_time": "2025-12-18T09:31:16.590747Z"
    }
   },
   "source": [
    "# ====== CONFIG ======\n",
    "OUTPUT_DIM = 128\n",
    "LR = 5e-4              \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "TEMPERATURE = 0.1\n",
    "SEED = 42\n",
    "D_EMB = len(next(iter(drug_embeddings_map.values())))\n",
    "print(\"Detected D_EMB =\", D_EMB)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def contrastive_loss_symmetric(emb_i, emb_j, temperature=TEMPERATURE):\n",
    "    def _loss(query, key):\n",
    "        query = F.normalize(query, dim=1)\n",
    "        key = F.normalize(key, dim=1)\n",
    "        sim_matrix = torch.matmul(query, key.T) / temperature\n",
    "        pos_mask = torch.eye(sim_matrix.size(0), dtype=torch.bool, device=query.device)\n",
    "        log_prob = F.log_softmax(sim_matrix, dim=1)\n",
    "        return -log_prob[pos_mask].mean()\n",
    "    \n",
    "    return (_loss(emb_i, emb_j) + _loss(emb_j, emb_i)) / 2\n",
    "\n",
    "class DrugEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=D_EMB, output_dim=OUTPUT_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Layer 1: Maintain dim, learn features\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            # Layer 2: Compress\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            # Layer 3: Project\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class ContrastiveDrugDataset(Dataset):\n",
    "    def __init__(self, embeddings_map, df_combo):\n",
    "        self.cids = list(embeddings_map.keys())\n",
    "        \n",
    "        print(\"Preprocessing: Applying Global Z-Score Whitening...\")\n",
    "        all_embs = torch.stack([torch.from_numpy(embeddings_map[c]) for c in self.cids]).float()\n",
    "        \n",
    "        # Calculate stats\n",
    "        mean = all_embs.mean(dim=0, keepdim=True)\n",
    "        std = all_embs.std(dim=0, keepdim=True) + 1e-6\n",
    "        \n",
    "        # Whiten\n",
    "        all_embs = (all_embs - mean) / std\n",
    "        \n",
    "        # Check integrity\n",
    "        if torch.isnan(all_embs).any():\n",
    "            raise ValueError(\"NaNs found after whitening!\")\n",
    "            \n",
    "        # Update mapping with whitened embeddings\n",
    "        whitened_map = {}\n",
    "        whitened_numpy = all_embs.numpy()\n",
    "        for i, cid in enumerate(self.cids):\n",
    "            whitened_map[cid] = whitened_numpy[i]\n",
    "            \n",
    "        self.cid_to_emb = whitened_map\n",
    "\n",
    "        print(f\"Processing combo data ({len(df_combo)} rows)...\")\n",
    "        valid_cids = set(self.cids)\n",
    "        valid_mask = df_combo['STITCH 1'].isin(valid_cids) & df_combo['STITCH 2'].isin(valid_cids)\n",
    "        df_valid = df_combo[valid_mask].copy()\n",
    "        \n",
    "        s1 = df_valid['STITCH 1'].values\n",
    "        s2 = df_valid['STITCH 2'].values\n",
    "        \n",
    "        # Canonical sort row-wise\n",
    "        min_s = np.minimum(s1, s2)\n",
    "        max_s = np.maximum(s1, s2)\n",
    "        \n",
    "        # Unique pairs\n",
    "        pairs = set(zip(min_s, max_s))\n",
    "        self.positive_pairs = list(pairs)\n",
    "        print(f\"Dataset ready. {len(self.positive_pairs)} positive pairs. {len(self.cids)} unique drugs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positive_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cid_a, cid_b = self.positive_pairs[idx]\n",
    "        emb_a = torch.tensor(self.cid_to_emb[cid_a], dtype=torch.float32)\n",
    "        emb_b = torch.tensor(self.cid_to_emb[cid_b], dtype=torch.float32)\n",
    "        return emb_a, emb_b\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected D_EMB = 2400\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "f9d3cec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:31:17.485434Z",
     "start_time": "2025-12-18T09:31:17.421550Z"
    }
   },
   "source": [
    "def contrastive_loss_train(drug_embeddings_map, df_combo):\n",
    "    dataset = ContrastiveDrugDataset(drug_embeddings_map, df_combo)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    print(\"\\n--- 2. TRAINING SETUP (Optimization V2) ---\")\n",
    "    print(\"Architecture: 3-Layer Deep MLP (BN+ReLU)\")\n",
    "    print(f\"Loss Temp: {TEMPERATURE}\")\n",
    "    model = DrugEncoder().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"LR: {LR}\")\n",
    "    \n",
    "    print(\"\\n--- 3. STARTING TRAINING (Input Whitened + Deep Projector) ---\")\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for emb_a, emb_b in dataloader:\n",
    "            emb_a, emb_b = emb_a.to(DEVICE), emb_b.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            z_a = model(emb_a)\n",
    "            z_b = model(emb_b)\n",
    "            \n",
    "            # Loss\n",
    "            loss = contrastive_loss_symmetric(z_a, z_b)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        avg_loss = total_loss / batch_count\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "\n",
    "    print(f\"\\nTraining Complete. Best Loss: {best_loss:.4f}\")\n",
    "    if best_loss < 4.10:\n",
    "         print(\"Success: Loss is consistently below random baseline (4.15).\")\n",
    "         \n",
    "    model.eval()\n",
    "    refined_map = {}\n",
    "    with torch.no_grad():\n",
    "        for cid, emb in dataset.cid_to_emb.items():\n",
    "            emb_tensor = torch.tensor(emb, dtype=torch.float32).to(DEVICE).unsqueeze(0)\n",
    "            refined = model(emb_tensor).cpu().numpy().flatten()\n",
    "            refined_map[cid] = refined\n",
    "        \n",
    "    return refined_map"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "edc478a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:31:35.288642Z",
     "start_time": "2025-12-18T09:31:33.134046Z"
    }
   },
   "source": [
    "file_path = base_path + 'bio-decagon-combo.csv'\n",
    "df_combo = pd.read_csv(file_path)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "6ff89b06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:35:34.953801Z",
     "start_time": "2025-12-18T09:31:36.569319Z"
    }
   },
   "source": [
    "refined_map = contrastive_loss_train(drug_embeddings_map, df_combo)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: Applying Global Z-Score Whitening...\n",
      "Processing combo data (4649441 rows)...\n",
      "Dataset ready. 63473 positive pairs. 645 unique drugs.\n",
      "\n",
      "--- 2. TRAINING SETUP (Optimization V2) ---\n",
      "Architecture: 3-Layer Deep MLP (BN+ReLU)\n",
      "Loss Temp: 0.1\n",
      "Device: cuda\n",
      "Batch Size: 64\n",
      "LR: 0.0005\n",
      "\n",
      "--- 3. STARTING TRAINING (Input Whitened + Deep Projector) ---\n",
      "Epoch 1/20, Loss: 4.1614\n",
      "Epoch 2/20, Loss: 4.1350\n",
      "Epoch 3/20, Loss: 4.1095\n",
      "Epoch 4/20, Loss: 4.0895\n",
      "Epoch 5/20, Loss: 4.0726\n",
      "Epoch 6/20, Loss: 4.0573\n",
      "Epoch 7/20, Loss: 4.0423\n",
      "Epoch 8/20, Loss: 4.0299\n",
      "Epoch 9/20, Loss: 4.0170\n",
      "Epoch 10/20, Loss: 4.0077\n",
      "Epoch 11/20, Loss: 3.9973\n",
      "Epoch 12/20, Loss: 3.9886\n",
      "Epoch 13/20, Loss: 3.9825\n",
      "Epoch 14/20, Loss: 3.9755\n",
      "Epoch 15/20, Loss: 3.9697\n",
      "Epoch 16/20, Loss: 3.9624\n",
      "Epoch 17/20, Loss: 3.9559\n",
      "Epoch 18/20, Loss: 3.9531\n",
      "Epoch 19/20, Loss: 3.9452\n",
      "Epoch 20/20, Loss: 3.9387\n",
      "\n",
      "Training Complete. Best Loss: 3.9387\n",
      "Success: Loss is consistently below random baseline (4.15).\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cd20fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved refined embeddings to 'refined_drug_embeddings_fixed.pkl'\n"
     ]
    }
   ],
   "source": [
    "output_file = base_path + 'refined_drug_embeddings_fixed.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(refined_map, f)\n",
    "\n",
    "print(f\"Saved refined embeddings to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c2b99",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ffde4fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:35:35.055818Z",
     "start_time": "2025-12-18T09:35:34.955058Z"
    }
   },
   "source": "D_EMB = 128\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass DrugPairDataset(Dataset):\n    def __init__(self, pairs: List[Tuple[str, str]], drug_embeddings_map: Dict[str, np.ndarray], labels: np.ndarray, is_binary: bool):\n        self.pairs = [tuple(p) for p in pairs]\n        self.cid_to_emb = drug_embeddings_map\n        self.labels = labels\n        self.is_binary = is_binary\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        cid_a, cid_b = self.pairs[idx]\n\n        emb_a = torch.tensor(self.cid_to_emb[cid_a], dtype=torch.float32)\n        emb_b = torch.tensor(self.cid_to_emb[cid_b], dtype=torch.float32)\n\n        y = self.labels[idx]\n        y = torch.as_tensor(y, dtype=torch.float32)\n        if self.is_binary:\n            y = y.squeeze().view(1)\n        else:\n            y = y.squeeze()\n\n        return emb_a, emb_b, y\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, embed_dim=D_EMB, num_output_classes=1, is_binary=True):\n        super(CrossAttentionFusion, self).__init__()\n\n        self.attn = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Deep MLP Head (512 -> 1024 -> 2048)\n        self.mlp_head = nn.Sequential(\n            # Layer 1: Input (embed_dim*2) -> 512\n            nn.Linear(embed_dim * 2, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(negative_slope=0.1),\n            nn.Dropout(0.2),\n\n            # Layer 2: 512 -> 1024\n            nn.Linear(512, 1024),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(negative_slope=0.1),\n            nn.Dropout(0.2),\n\n            # Layer 3: 1024 -> 2048\n            nn.Linear(1024, 2048),\n            nn.BatchNorm1d(2048),\n            nn.LeakyReLU(negative_slope=0.1),\n            nn.Dropout(0.2),\n\n            # Output Layer: 2048 -> Num Classes\n            nn.Linear(2048, num_output_classes)\n        )\n\n        self.is_binary = is_binary\n\n    def forward(self, emb_a, emb_b):\n        # 1. A queries B (A->B interaction)\n        attn_out_a, _ = self.attn(\n            query=emb_a.unsqueeze(1),\n            key=emb_b.unsqueeze(1),\n            value=emb_b.unsqueeze(1)\n        )\n\n        # 2. B queries A (B->A interaction)\n        attn_out_b, _ = self.attn(\n            query=emb_b.unsqueeze(1),\n            key=emb_a.unsqueeze(1),\n            value=emb_a.unsqueeze(1)\n        )\n\n        fused_vector = torch.cat([attn_out_a.squeeze(1), attn_out_b.squeeze(1)], dim=1)\n\n        logits = self.mlp_head(fused_vector)\n\n        # Return logits (no sigmoid) for use with BCEWithLogitsLoss\n        return logits\n\nclass FocalLoss(nn.Module):\n\n    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        # Use binary_cross_entropy_with_logits which is autocast-safe\n        # inputs should be logits (before sigmoid)\n        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n\n        # Calculate Pt (probability of correct class)\n        # For BCEWithLogits, we need to manually compute probabilities\n        probs = torch.sigmoid(inputs)\n        Pt = targets * probs + (1 - targets) * (1 - probs)\n\n        Focal_Term = (1 - Pt) ** self.gamma\n\n        alpha_term = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n\n        focal_loss = alpha_term * Focal_Term * BCE_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\ndef save_checkpoint(model, optimizer, epoch, metrics, filename):\n    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'metrics': metrics}, filename)\n    print(f\"Checkpoint saved to {filename}\")\n\ndef save_results(results_df, filename=\"experiment_results_grover.csv\"):\n    if os.path.exists(filename):\n        existing_df = pd.read_csv(filename)\n        results_df = pd.concat([existing_df, results_df], ignore_index=True)\n    results_df.to_csv(filename, index=False)\n    print(f\"\\n--- Results saved to {filename} ---\")\n\ndef train_model(model, train_loader, optimizer, criterion, scaler):\n    model.train()\n    total_loss = 0\n    for emb_a, emb_b, labels in train_loader:\n        emb_a, emb_b, labels = emb_a.to(DEVICE), emb_b.to(DEVICE), labels.to(DEVICE)\n\n        optimizer.zero_grad()\n        \n        # Use autocast for mixed precision training\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            predictions = model(emb_a, emb_b)\n            loss = criterion(predictions, labels)\n        \n        # Scale loss and backward\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n    return total_loss / len(train_loader)\n\ndef evaluate_model(model, data_loader, is_binary):\n    model.eval()\n    all_preds_proba = []\n    all_labels = []\n\n    with torch.no_grad():\n        for emb_a, emb_b, labels in data_loader:\n            emb_a, emb_b = emb_a.to(DEVICE), emb_b.to(DEVICE)\n\n            # Get logits and apply sigmoid to get probabilities\n            logits = model(emb_a, emb_b)\n            preds_tensor = torch.sigmoid(logits).cpu().detach()\n            preds = np.array(preds_tensor.tolist())\n            all_preds_proba.extend(preds)\n\n            all_labels.extend(labels.cpu().tolist())\n\n    all_preds_proba = np.array(all_preds_proba)\n    all_labels = np.array(all_labels)\n\n    if is_binary:\n        metric_score = roc_auc_score(all_labels.squeeze(), all_preds_proba.squeeze())\n        return metric_score, 'AUROC'\n    else:\n        # Multi-Label Task Metric: F1-Micro\n        all_preds = (all_preds_proba > 0.5).astype(int)\n        metric_score = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n        return metric_score, 'F1-Micro'\n\n\ndef prepare_data_hierarchical(embeddings_file: str, combo_file: str, map_file: str, is_binary_task: bool):\n\n    print(f\"1. Loading Refined embeddings from {embeddings_file}...\")\n    with open(embeddings_file, 'rb') as f:\n        grover_emb_map = pickle.load(f)\n    all_drugs = set(grover_emb_map.keys())\n    print(f\"   -> Loaded {len(all_drugs)} unique drug embeddings.\")\n\n    df_combo = pd.read_csv(combo_file)\n\n    print(\"Filtering side effects (>= 500 occurrences)...\")\n    se_col = 'Polypharmacy Side Effect'\n    if se_col in df_combo.columns:\n        se_counts = df_combo[se_col].value_counts()\n        valid_se = se_counts[se_counts >= 500].index\n        print(f\"Original unique side effects: {len(se_counts)}\")\n        print(f\"Keeping {len(valid_se)} unique side effects.\")\n        df_combo = df_combo[df_combo[se_col].isin(valid_se)]\n        print(f\"Filtered dataset size: {len(df_combo)} rows.\")\n    else:\n        print(f\"Warning: Column '{se_col}' not found. Skipping filtering.\")\n\n    print(f\"2. Loading hierarchical map from {map_file}...\")\n    df_map = pd.read_csv(map_file)\n\n    FINE_GRAINED_COL = 'Side Effect Name'\n    COARSE_GRAINED_COL = 'Disease Class'\n\n    side_effect_to_parent_map = pd.Series(\n        df_map[COARSE_GRAINED_COL].values,\n        index=df_map[FINE_GRAINED_COL]\n    ).to_dict()\n    print(f\"   -> Map built: {len(side_effect_to_parent_map)} fine-grained effects mapped to categories.\")\n\n    pair_to_side_effects_map: Dict[frozenset, Set[str]] = {}\n\n    for _, row in df_combo.iterrows():\n        id_a = str(row['STITCH 1']).strip()\n        id_b = str(row['STITCH 2']).strip()\n        side_effect_code = row['Side Effect Name']\n\n        if id_a in all_drugs and id_b in all_drugs:\n            pair = frozenset([id_a, id_b])\n\n            if pair not in pair_to_side_effects_map:\n                pair_to_side_effects_map[pair] = set()\n            pair_to_side_effects_map[pair].add(side_effect_code)\n\n    print(f\"3. Found {len(pair_to_side_effects_map)} unique, embedded drug pairs.\")\n\n    all_pairs: List[frozenset] = list(pair_to_side_effects_map.keys())\n    all_labels_list_of_sets = [pair_to_side_effects_map[pair] for pair in all_pairs]\n\n    coarse_grained_labels_list_of_sets = []\n    all_original_se_codes = set()\n\n    for se_set in all_labels_list_of_sets:\n        coarse_set = set()\n        for se_code in se_set:\n            all_original_se_codes.add(se_code)\n            parent_se = side_effect_to_parent_map.get(se_code, se_code)\n            coarse_set.add(parent_se)\n\n        coarse_grained_labels_list_of_sets.append(coarse_set)\n\n    total_initial_classes = len(all_original_se_codes)\n\n    mlb = MultiLabelBinarizer()\n    y = mlb.fit_transform(coarse_grained_labels_list_of_sets)\n\n    N_CLASSES = len(mlb.classes_)\n    print(f\"   -> Original # of Classes: {total_initial_classes}\")\n    print(f\"   -> New # of Classes: {N_CLASSES}\")\n\n    X_train, X_temp, y_train, y_temp = train_test_split(\n            all_pairs, y, test_size=0.2, random_state=42\n        )\n\n    X_val, X_test, y_val, y_test = train_test_split(\n            X_temp, y_temp, test_size=0.5, random_state=42\n        )\n\n    print(f\"   -> Split Stats: Train={len(X_train)} (80%), Val={len(X_val)} (10%), Test={len(X_test)} (10%)\")\n\n    return X_train, X_val, X_test, y_train, y_val, y_test, grover_emb_map, N_CLASSES\n\n\n\ndef prepare_data(embeddings_file: str, combo_file: str, is_binary_task: bool):\n    print(f\"1. Loading GROVER embeddings from {embeddings_file}...\")\n    with open(embeddings_file, 'rb') as f:\n        # Load embeddings into grover_emb_map\n        grover_emb_map = pickle.load(f)\n    all_drugs = set(grover_emb_map.keys())\n    print(f\"   -> Loaded {len(all_drugs)} unique drug embeddings.\")\n\n    df_combo = pd.read_csv(combo_file)\n\n    print(\"Filtering side effects (>= 500 occurrences)...\")\n    se_col = 'Polypharmacy Side Effect'\n    if se_col in df_combo.columns:\n        se_counts = df_combo[se_col].value_counts()\n        valid_se = se_counts[se_counts >= 500].index\n        print(f\"Original unique side effects: {len(se_counts)}\")\n        print(f\"Keeping {len(valid_se)} unique side effects.\")\n        df_combo = df_combo[df_combo[se_col].isin(valid_se)]\n        print(f\"Filtered dataset size: {len(df_combo)} rows.\")\n    else:\n        print(f\"Warning: Column '{se_col}' not found. Skipping filtering.\")\n\n    # --- BINARY TASK PATH ---\n    if is_binary_task:\n        print(\"   -> Preparing data for BINARY classification (Drug Interaction Existence)...\")\n        positive_pairs = set()\n        for _, row in df_combo.iterrows():\n            # Use STITCH IDs, sorted to ensure unique pairs\n            d1 = str(row['STITCH 1']).strip()\n            d2 = str(row['STITCH 2']).strip()\n\n            # Ensure both drugs are in our embedding map\n            if d1 in grover_emb_map and d2 in grover_emb_map:\n                pair = tuple(sorted([d1, d2]))\n                positive_pairs.add(pair)\n\n        all_pairs = list(positive_pairs)\n        n_pos = len(all_pairs)\n        print(f\"   -> Found {n_pos} positive pairs.\")\n\n        # Generate Negative Samples (1:1 Ratio) by random sampling\n        np.random.seed(42)\n        all_drugs_list = list(all_drugs)\n\n        while len(all_pairs) < 2 * n_pos:\n            d1, d2 = np.random.choice(all_drugs_list, 2, replace=False)\n            pair = tuple(sorted([d1, d2]))\n            if pair not in positive_pairs:\n                all_pairs.append(pair)\n\n        # Labels: 1 for positives, 0 for negatives\n        labels = np.array([1] * n_pos + [0] * n_pos)\n\n        # 80/10/10 Split for Binary\n        # 1. Train (80%) vs Temp (20%)\n        X_train_pairs, X_temp_pairs, y_train, y_temp = train_test_split(\n            all_pairs, labels, test_size=0.2, random_state=42, stratify=labels\n        )\n\n        # 2. Temp (20%) -> Val (10%) + Test (10%)\n        X_val_pairs, X_test_pairs, y_val, y_test = train_test_split(\n            X_temp_pairs, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n        )\n\n        print(f\"   -> Binary Split Stats: Train={len(X_train_pairs)}, Val={len(X_val_pairs)}, Test={len(X_test_pairs)}\")\n\n        # Reshape labels for compatibility\n        return X_train_pairs, X_val_pairs, X_test_pairs, y_train.reshape(-1, 1), y_val.reshape(-1, 1), y_test.reshape(-1, 1), grover_emb_map\n\n    # --- MULTI-LABEL TASK PATH ---\n    print(\"   -> Preparing data for MULTI-LABEL classification...\")\n    pair_to_side_effects_map: Dict[frozenset, Set[str]] = {}\n\n    for _, row in df_combo.iterrows():\n        id_a = str(row['STITCH 1']).strip()\n        id_b = str(row['STITCH 2']).strip()\n        side_effect = row['Polypharmacy Side Effect']\n\n        if id_a in all_drugs and id_b in all_drugs:\n            pair = frozenset([id_a, id_b])\n\n            if pair not in pair_to_side_effects_map:\n                pair_to_side_effects_map[pair] = set()\n            pair_to_side_effects_map[pair].add(side_effect)\n\n    print(f\"2. Found {len(pair_to_side_effects_map)} unique, embedded drug pairs.\")\n\n    all_pairs: List[frozenset] = list(pair_to_side_effects_map.keys())\n    all_labels_list_of_sets = [pair_to_side_effects_map[pair] for pair in all_pairs]\n\n    mlb = MultiLabelBinarizer()\n    y = mlb.fit_transform(all_labels_list_of_sets)\n\n    N_CLASSES = len(mlb.classes_)\n    print(f\"   -> MultiLabelBinarizer found {N_CLASSES} total unique side effects.\")\n\n    # 80/10/10 Split for Multi-Label\n    X_train, X_temp, y_train, y_temp = train_test_split(\n        all_pairs, y, test_size=0.2, random_state=42\n    )\n\n    X_val, X_test, y_val, y_test = train_test_split(\n        X_temp, y_temp, test_size=0.5, random_state=42\n    )\n\n    print(f\"   -> Split Stats: Train={len(X_train)} (80%), Val={len(X_val)} (10%), Test={len(X_test)} (10%)\")\n\n    return X_train, X_val, X_test, y_train, y_val, y_test, grover_emb_map, N_CLASSES",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "45a537c5",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "code",
   "id": "db0ec7cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:35:35.102701Z",
     "start_time": "2025-12-18T09:35:35.057041Z"
    }
   },
   "source": "def run_binary_training(embeddings_file, combo_file, epochs=20):\n    print(\"\\n--- Starting Binary Classification with Cross-Attention (Balanced) ---\")\n    \n    # 1. Data Setup (Balanced, 80/10/10)\n    # This prepares 3 sets: Train (80%), Validation (10%), Test (10%)\n    X_train_pairs, X_val_pairs, X_test_pairs, y_train, y_val, y_test, emb_map = prepare_data(embeddings_file, combo_file, is_binary_task=True)\n\n    train_dataset = DrugPairDataset(X_train_pairs, emb_map, y_train, is_binary=True)\n    val_dataset = DrugPairDataset(X_val_pairs, emb_map, y_val, is_binary=True)\n    test_dataset = DrugPairDataset(X_test_pairs, emb_map, y_test, is_binary=True)\n\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    # 2. Model Setup\n    model = CrossAttentionFusion(embed_dim=D_EMB, num_output_classes=1, is_binary=True).to(DEVICE)\n\n    criterion = FocalLoss(alpha=0.25, gamma=2).to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    \n    # Create GradScaler for mixed precision training\n    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n\n    best_metric = 0.0\n    task_name = \"binary_grover\"\n\n    # 3. Training and Saving Checkpoint\n    for epoch in range(epochs):\n        train_loss = train_model(model, train_loader, optimizer, criterion, scaler)\n\n        # Evaluate on Validation Set for model selection\n        current_metric, metric_name = evaluate_model(model, val_loader, is_binary=True)\n\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val {metric_name}: {current_metric:.4f}\")\n\n        if current_metric > best_metric:\n            best_metric = current_metric\n            print(f\"  --> New best model saved (Val)! {metric_name}: {best_metric:.4f}\")\n            save_checkpoint(\n                model,\n                optimizer,\n                epoch,\n                {metric_name: current_metric},\n                filename=base_path + f'{task_name}_best_model.pth'\n            )\n\n    # Final Test Eval using the Best Saved Model\n    print(\"\\n--- Final Evaluation on TEST SET (Using Best Validation Model) ---\")\n    if os.path.exists(base_path + f'{task_name}_best_model.pth'):\n        checkpoint = torch.load(base_path + f'{task_name}_best_model.pth', map_location=DEVICE)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n\n    model.eval()\n    all_preds_proba, all_labels = [], []\n    with torch.no_grad():\n        for emb_a, emb_b, labels in test_loader:\n            # Move tensors to device\n            emb_a, emb_b = emb_a.to(DEVICE), emb_b.to(DEVICE)\n            \n            # Get logits and apply sigmoid\n            logits = model(emb_a, emb_b)\n            preds = torch.sigmoid(logits).squeeze().cpu().numpy()\n            all_preds_proba.extend(preds)\n            all_labels.extend(labels.squeeze().cpu().numpy())\n\n    auroc = roc_auc_score(all_labels, all_preds_proba)\n    aupr = average_precision_score(all_labels, all_preds_proba)\n\n    print(\"\\n--- Binary Model Final Evaluation (Test Set) ---\")\n    print(f\"AUROC (Area Under ROC Curve): {auroc:.4f}\")\n    print(f\"AUPR (Average Precision-Recall): {aupr:.4f}\")\n\n    results_df = pd.DataFrame({\n        'Task': [task_name],\n        'Model': ['Cross-Attention'],\n        'Epochs': [epochs],\n        'AUROC': [auroc],\n        'AUPR': [aupr],\n        'Best_Metric': [best_metric]\n    })\n    save_results(results_df, base_path + 'experiment_results_grover.csv')",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "542ad13e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:50:38.563668Z",
     "start_time": "2025-12-18T09:35:35.104133Z"
    }
   },
   "source": [
    "EMBEDDINGS_FILE = base_path + 'refined_drug_embeddings_fixed.pkl'\n",
    "COMBO_FILE = base_path + 'bio-decagon-combo.csv'\n",
    "NUM_EPOCHS = 20\n",
    "print(\"\\n[PHASE 1] Running Binary Classification (Balanced Data)\")\n",
    "start_time_binary = time.time()\n",
    "    \n",
    "run_binary_training(\n",
    "        embeddings_file=EMBEDDINGS_FILE, \n",
    "        combo_file=COMBO_FILE, \n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "    \n",
    "end_time_binary = time.time()\n",
    "print(f\"\\n✅ PHASE 1 Complete. Time taken: {end_time_binary - start_time_binary:.2f} seconds.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PHASE 1] Running Binary Classification (Balanced Data)\n",
      "\n",
      "--- Starting Binary Classification with Cross-Attention (Balanced) ---\n",
      "1. Loading GROVER embeddings from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/refined_drug_embeddings_fixed.pkl...\n",
      "   -> Loaded 645 unique drug embeddings.\n",
      "Filtering side effects (>= 500 occurrences)...\n",
      "Original unique side effects: 1317\n",
      "Keeping 963 unique side effects.\n",
      "Filtered dataset size: 4576287 rows.\n",
      "   -> Preparing data for BINARY classification (Drug Interaction Existence)...\n",
      "   -> Found 63472 positive pairs.\n",
      "   -> Binary Split Stats: Train=101555, Val=12694, Test=12695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3331082965.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.0615 | Val AUROC: 0.8465\n",
      "  --> New best model saved (Val)! AUROC: 0.8465\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Loss: 0.0529 | Val AUROC: 0.8739\n",
      "  --> New best model saved (Val)! AUROC: 0.8739\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Loss: 0.0498 | Val AUROC: 0.9019\n",
      "  --> New best model saved (Val)! AUROC: 0.9019\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Loss: 0.0476 | Val AUROC: 0.9016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Loss: 0.0457 | Val AUROC: 0.9123\n",
      "  --> New best model saved (Val)! AUROC: 0.9123\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Loss: 0.0445 | Val AUROC: 0.9176\n",
      "  --> New best model saved (Val)! AUROC: 0.9176\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Loss: 0.0435 | Val AUROC: 0.9222\n",
      "  --> New best model saved (Val)! AUROC: 0.9222\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Loss: 0.0426 | Val AUROC: 0.9280\n",
      "  --> New best model saved (Val)! AUROC: 0.9280\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train Loss: 0.0416 | Val AUROC: 0.9300\n",
      "  --> New best model saved (Val)! AUROC: 0.9300\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train Loss: 0.0411 | Val AUROC: 0.9298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train Loss: 0.0403 | Val AUROC: 0.9366\n",
      "  --> New best model saved (Val)! AUROC: 0.9366\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train Loss: 0.0399 | Val AUROC: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train Loss: 0.0394 | Val AUROC: 0.9397\n",
      "  --> New best model saved (Val)! AUROC: 0.9397\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train Loss: 0.0388 | Val AUROC: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train Loss: 0.0383 | Val AUROC: 0.9395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train Loss: 0.0379 | Val AUROC: 0.9404\n",
      "  --> New best model saved (Val)! AUROC: 0.9404\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train Loss: 0.0374 | Val AUROC: 0.9419\n",
      "  --> New best model saved (Val)! AUROC: 0.9419\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Train Loss: 0.0371 | Val AUROC: 0.9380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Train Loss: 0.0366 | Val AUROC: 0.9422\n",
      "  --> New best model saved (Val)! AUROC: 0.9422\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Train Loss: 0.0363 | Val AUROC: 0.9432\n",
      "  --> New best model saved (Val)! AUROC: 0.9432\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/binary_grover_best_model.pth\n",
      "\n",
      "--- Final Evaluation on TEST SET (Using Best Validation Model) ---\n",
      "Loaded best model from epoch 19\n",
      "\n",
      "--- Binary Model Final Evaluation (Test Set) ---\n",
      "AUROC (Area Under ROC Curve): 0.9435\n",
      "AUPR (Average Precision-Recall): 0.9455\n",
      "\n",
      "--- Results saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/experiment_results_grover.csv ---\n",
      "\n",
      "✅ PHASE 1 Complete. Time taken: 903.34 seconds.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "4f10b46d",
   "metadata": {},
   "source": [
    "### Multi_label"
   ]
  },
  {
   "cell_type": "code",
   "id": "820f197d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:50:38.715339Z",
     "start_time": "2025-12-18T09:50:38.661678Z"
    }
   },
   "source": "def evaluate_saved_model(checkpoint_path, embeddings_file, combo_file, hierarchical=False):\n    print(f\"\\n--- Evaluating Saved Model: {checkpoint_path} ---\")\n    \n    # 1. Data Setup (Only need Test Set)\n    if hierarchical:\n        results = prepare_data_hierarchical(\n            embeddings_file, \n            combo_file, \n            map_file=base_path + 'bio-decagon-effectcategories.csv',\n            is_binary_task=False\n        )\n        X_train_pairs, X_val_pairs, X_test_pairs, y_train, y_val, y_test, emb_map, N_CLASSES = results\n    else:\n        results = prepare_data(embeddings_file, combo_file, is_binary_task=False)\n        X_train_pairs, X_val_pairs, X_test_pairs, y_train, y_val, y_test, emb_map, N_CLASSES = results\n    \n    test_dataset = DrugPairDataset(X_test_pairs, emb_map, y_test, is_binary=False)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n    \n    # 2. Model Setup\n    model = CrossAttentionFusion(embed_dim=D_EMB, num_output_classes=N_CLASSES, is_binary=False).to(DEVICE)\n    \n    # 3. Load Checkpoint\n    if os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded weights from epoch {checkpoint['epoch']}\")\n    else:\n        print(f\"Error: Checkpoint {checkpoint_path} not found.\")\n        return\n\n    # 4. Evaluation Loop\n    model.eval()\n    all_preds_proba, all_labels = [], []\n    with torch.no_grad():\n        for emb_a, emb_b, labels in test_loader:\n            emb_a, emb_b = emb_a.to(DEVICE), emb_b.to(DEVICE)\n            \n            # Get logits and apply sigmoid\n            logits = model(emb_a, emb_b)\n            preds_tensor = torch.sigmoid(logits).cpu().detach()\n            preds = np.array(preds_tensor.tolist())\n            all_preds_proba.extend(preds)\n            \n            all_labels.extend(labels.cpu().tolist())\n    \n    all_preds_proba = np.array(all_preds_proba)\n    all_labels = np.array(all_labels)\n    all_preds = (all_preds_proba > 0.5).astype(int)\n    \n    # --- METRICS ---\n    # F1 Scores\n    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    \n    # AUROC\n    try:\n        auroc_micro = roc_auc_score(all_labels, all_preds_proba, average='micro')\n        auroc_macro = roc_auc_score(all_labels, all_preds_proba, average='macro')\n    except Exception:\n        auroc_micro, auroc_macro = 0.0, 0.0\n    \n    # AUPR\n    try:\n        aupr_micro = average_precision_score(all_labels, all_preds_proba, average='micro')\n        aupr_macro = average_precision_score(all_labels, all_preds_proba, average='macro')\n    except Exception:\n        aupr_micro, aupr_macro = 0.0, 0.0\n        \n    # P@50 (Requires get_precision_at_k helper)\n    try:\n        p_at_50 = get_precision_at_k(all_labels, all_preds_proba, k=50)\n    except Exception:\n        p_at_50 = 0.0\n    \n    print(\"\\n--- Final Evaluation Metrics (Saved Model) ---\")\n    print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n    print(f\"AUROC (Micro):    {auroc_micro:.4f}\")\n    print(f\"AUROC (Macro):    {auroc_macro:.4f}\")\n    print(f\"AUPR (Micro):     {aupr_micro:.4f}\")\n    print(f\"AUPR (Macro):     {aupr_macro:.4f}\")\n    print(f\"P@50:             {p_at_50:.4f}\")",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:50:38.816943Z",
     "start_time": "2025-12-18T09:50:38.716923Z"
    }
   },
   "cell_type": "code",
   "source": "def get_precision_at_k(y_true, y_prob, k=50):\n    # y_true: binary labels (N, C)\n    # y_prob: predicted probabilities (N, C)\n    top_k_indices = np.argsort(y_prob, axis=1)[:, ::-1][:, :k]\n    precision_sum = 0.0\n    n_samples = y_true.shape[0]\n    for i in range(n_samples):\n        true_labels = y_true[i]\n        pred_indices = top_k_indices[i]\n        num_correct = np.sum(true_labels[pred_indices])\n        precision_at_k = num_correct / k\n        precision_sum += precision_at_k\n    return precision_sum / n_samples",
   "id": "355b2033e4be2566",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:50:38.900514Z",
     "start_time": "2025-12-18T09:50:38.818692Z"
    }
   },
   "cell_type": "code",
   "source": "def run_multi_label_training(embeddings_file, combo_file, epochs=20, hierarchical=False):\n    print(\"\\n--- Starting Multi-Label Classification with Cross-Attention (Long-Tailed) ---\")\n\n    # 1. Data Setup (Long-Tailed/Unbalanced)\n    # Both prepare_data functions now return 8 values (3 sets of X/y + map + classes)\n    if hierarchical:\n        results = prepare_data_hierarchical(\n            embeddings_file,\n            combo_file,\n            map_file=base_path + 'bio-decagon-effectcategories.csv',\n            is_binary_task=False\n        )\n        # Unpack 3 sets\n        X_train_pairs, X_val_pairs, X_test_pairs, y_train, y_val, y_test, emb_map, N_CLASSES = results\n    else:\n        results = prepare_data(embeddings_file, combo_file, is_binary_task=False)\n        X_train_pairs, X_val_pairs, X_test_pairs, y_train, y_val, y_test, emb_map, N_CLASSES = results\n\n    print(f\"   -> Training on {len(X_train_pairs)} samples\")\n    print(f\"   -> Validating on {len(X_val_pairs)} samples\")\n    print(f\"   -> Testing on {len(X_test_pairs)} samples\")\n\n    train_dataset = DrugPairDataset(X_train_pairs, emb_map, y_train, is_binary=False)\n    val_dataset = DrugPairDataset(X_val_pairs, emb_map, y_val, is_binary=False)\n    test_dataset = DrugPairDataset(X_test_pairs, emb_map, y_test, is_binary=False)\n\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    # 2. Model Setup\n    model = CrossAttentionFusion(embed_dim=D_EMB, num_output_classes=N_CLASSES, is_binary=False).to(DEVICE)\n\n    criterion = FocalLoss(alpha=0.25, gamma=2).to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    \n    # Create GradScaler for mixed precision training\n    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n\n    best_metric = 0.0\n    if hierarchical:\n        task_name = \"multi_label_hierarchical_grover\"\n    else:\n        task_name = \"multi_label_focal_grover\"\n\n    # 3. Training and Saving Checkpoint (Using Validation Set)\n    for epoch in range(epochs):\n        train_loss = train_model(model, train_loader, optimizer, criterion, scaler)\n\n        # Evaluate on VALIDATION set\n        current_metric, metric_name = evaluate_model(model, val_loader, is_binary=False)\n\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val {metric_name}: {current_metric:.4f}\")\n\n        if current_metric > best_metric:\n            best_metric = current_metric\n            print(f\"  --> New best model saved (Val)! {metric_name}: {best_metric:.4f}\")\n            save_checkpoint(\n                model,\n                optimizer,\n                epoch,\n                {metric_name: current_metric},\n                filename=base_path + f'{task_name}_best_model.pth'\n            )\n\n    # 4. Final Evaluation on TEST Set (loading best model)\n    print(\"\\n--- Final Evaluation on TEST SET (Using Best Validation Model) ---\")\n    if os.path.exists(base_path + f'{task_name}_best_model.pth'):\n        checkpoint = torch.load(base_path + f'{task_name}_best_model.pth', map_location=DEVICE)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n\n    model.eval()\n    all_preds_proba, all_labels = [], []\n    with torch.no_grad():\n        for emb_a, emb_b, labels in test_loader:\n            # Move tensors to device\n            emb_a, emb_b = emb_a.to(DEVICE), emb_b.to(DEVICE)\n\n            # Get logits and apply sigmoid\n            logits = model(emb_a, emb_b)\n            preds_tensor = torch.sigmoid(logits).cpu().detach()\n            preds = np.array(preds_tensor.tolist())\n            all_preds_proba.extend(preds)\n\n            all_labels.extend(labels.cpu().tolist())\n\n    all_preds_proba = np.array(all_preds_proba)\n    all_labels = np.array(all_labels)\n    all_preds = (all_preds_proba > 0.5).astype(int)\n\n    # --- METRICS CALCULATION ---\n    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n\n    try:\n        auroc_micro = roc_auc_score(all_labels, all_preds_proba, average='micro')\n    except Exception as e:\n        print(f\"Warning: AUROC failed: {e}\")\n        auroc_micro = 0.0\n\n    try:\n        aupr_micro = average_precision_score(all_labels, all_preds_proba, average='micro')\n    except Exception as e:\n        print(f\"Warning: AUPR failed: {e}\")\n        aupr_micro = 0.0\n\n    # Calculate P@50 using helper\n    try:\n        p_at_50 = get_precision_at_k(all_labels, all_preds_proba, k=50)\n    except Exception as e:\n        print(f\"Warning: P@50 failed: {e}\")\n        p_at_50 = 0.0\n\n    print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n    print(f\"AUROC (Micro):    {auroc_micro:.4f}\")\n    print(f\"AUPR (Micro):     {aupr_micro:.4f}\")\n    print(f\"P@50:             {p_at_50:.4f}\")\n\n    results_df = pd.DataFrame({\n        'Task': [task_name],\n        'Model': ['Cross-Attention'],\n        'Epochs': [epochs],\n        'AUROC': [auroc_micro],\n        'AUPR': [aupr_micro],\n        'P@50': [p_at_50],\n        'F1_Micro': [f1_micro],\n        'F1_Macro': [f1_macro],\n        'Best_Metric': [best_metric]\n    })\n    if hierarchical:\n        save_results(results_df, base_path + 'experiment_results_grover_hierarchical.csv')\n    else:\n        save_results(results_df, base_path + 'experiment_results_grover.csv')",
   "id": "b1b99bf3c88b50bc",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "f5b68251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:59:31.662046Z",
     "start_time": "2025-12-18T09:50:38.902745Z"
    }
   },
   "source": [
    "EMBEDDINGS_FILE = base_path + 'refined_drug_embeddings_fixed.pkl'\n",
    "COMBO_FILE = base_path + 'bio-decagon-combo.csv'\n",
    "NUM_EPOCH = 20\n",
    "print(\"\\n[PHASE 2] Running Multi-Label Classification (Balanced Data)\")\n",
    "start_time_binary = time.time()\n",
    "    \n",
    "run_multi_label_training(\n",
    "        embeddings_file=EMBEDDINGS_FILE, \n",
    "        combo_file=COMBO_FILE, \n",
    "        epochs=NUM_EPOCHS,\n",
    "        hierarchical=False\n",
    "    )\n",
    "    \n",
    "end_time_binary = time.time()\n",
    "print(f\"\\n✅ PHASE 2 Complete. Time taken: {end_time_binary - start_time_binary:.2f} seconds.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PHASE 2] Running Multi-Label Classification (Balanced Data)\n",
      "\n",
      "--- Starting Multi-Label Classification with Cross-Attention (Long-Tailed) ---\n",
      "1. Loading GROVER embeddings from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/refined_drug_embeddings_fixed.pkl...\n",
      "   -> Loaded 645 unique drug embeddings.\n",
      "Filtering side effects (>= 500 occurrences)...\n",
      "Original unique side effects: 1317\n",
      "Keeping 963 unique side effects.\n",
      "Filtered dataset size: 4576287 rows.\n",
      "   -> Preparing data for MULTI-LABEL classification...\n",
      "2. Found 63472 unique, embedded drug pairs.\n",
      "   -> MultiLabelBinarizer found 963 total unique side effects.\n",
      "   -> Split Stats: Train=50777 (80%), Val=6347 (10%), Test=6348 (10%)\n",
      "   -> Training on 50777 samples\n",
      "   -> Validating on 6347 samples\n",
      "   -> Testing on 6348 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/2057218711.py:38: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.0231 | Val F1-Micro: 0.0005\n",
      "  --> New best model saved (Val)! F1-Micro: 0.0005\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_focal_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Loss: 0.0212 | Val F1-Micro: 0.0019\n",
      "  --> New best model saved (Val)! F1-Micro: 0.0019\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_focal_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Loss: 0.0209 | Val F1-Micro: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Loss: 0.0206 | Val F1-Micro: 0.0046\n",
      "  --> New best model saved (Val)! F1-Micro: 0.0046\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_focal_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Loss: 0.0204 | Val F1-Micro: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Loss: 0.0202 | Val F1-Micro: 0.0129\n",
      "  --> New best model saved (Val)! F1-Micro: 0.0129\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_focal_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Loss: 0.0201 | Val F1-Micro: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Loss: 0.0200 | Val F1-Micro: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train Loss: 0.0199 | Val F1-Micro: 0.0155\n",
      "  --> New best model saved (Val)! F1-Micro: 0.0155\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_focal_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train Loss: 0.0198 | Val F1-Micro: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train Loss: 0.0197 | Val F1-Micro: 0.0095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train Loss: 0.0196 | Val F1-Micro: 0.0082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train Loss: 0.0195 | Val F1-Micro: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train Loss: 0.0195 | Val F1-Micro: 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train Loss: 0.0194 | Val F1-Micro: 0.0156\n",
      "  --> New best model saved (Val)! F1-Micro: 0.0156\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_focal_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train Loss: 0.0193 | Val F1-Micro: 0.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train Loss: 0.0193 | Val F1-Micro: 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Train Loss: 0.0192 | Val F1-Micro: 0.0180\n",
      "  --> New best model saved (Val)! F1-Micro: 0.0180\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_focal_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Train Loss: 0.0192 | Val F1-Micro: 0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Train Loss: 0.0191 | Val F1-Micro: 0.0140\n",
      "\n",
      "--- Final Evaluation on TEST SET (Using Best Validation Model) ---\n",
      "Loaded best model from epoch 17\n",
      "F1 Score (Micro): 0.0199\n",
      "F1 Score (Macro): 0.0058\n",
      "AUROC (Micro):    0.8693\n",
      "AUPR (Micro):     0.3808\n",
      "P@50:             0.3557\n",
      "\n",
      "--- Results saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/experiment_results_grover.csv ---\n",
      "\n",
      "✅ PHASE 2 Complete. Time taken: 532.61 seconds.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "5d036b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T10:23:02.457155Z",
     "start_time": "2025-12-18T10:21:18.433357Z"
    }
   },
   "source": [
    "metrics = evaluate_saved_model(\n",
    "    checkpoint_path=base_path+'multi_label_focal_grover_best_model.pth',\n",
    "    embeddings_file=base_path+'refined_drug_embeddings_fixed.pkl',\n",
    "    combo_file=base_path+'bio-decagon-combo.csv',\n",
    "    hierarchical=False  # Set to True if using hierarchical data\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Saved Model: /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_focal_grover_best_model.pth ---\n",
      "1. Loading GROVER embeddings from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/refined_drug_embeddings_fixed.pkl...\n",
      "   -> Loaded 645 unique drug embeddings.\n",
      "Filtering side effects (>= 500 occurrences)...\n",
      "Original unique side effects: 1317\n",
      "Keeping 963 unique side effects.\n",
      "Filtered dataset size: 4576287 rows.\n",
      "   -> Preparing data for MULTI-LABEL classification...\n",
      "2. Found 63472 unique, embedded drug pairs.\n",
      "   -> MultiLabelBinarizer found 963 total unique side effects.\n",
      "   -> Split Stats: Train=50777 (80%), Val=6347 (10%), Test=6348 (10%)\n",
      "Loaded weights from epoch 17\n",
      "\n",
      "--- Final Evaluation Metrics (Saved Model) ---\n",
      "F1 Score (Micro): 0.0199\n",
      "F1 Score (Macro): 0.0058\n",
      "AUROC (Micro):    0.8693\n",
      "AUROC (Macro):    0.8206\n",
      "AUPR (Micro):     0.3808\n",
      "AUPR (Macro):     0.2627\n",
      "P@50:             0.3557\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "41b8a8ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T10:10:24.453180Z",
     "start_time": "2025-12-18T10:01:18.963897Z"
    }
   },
   "source": [
    "EMBEDDINGS_FILE = base_path+'refined_drug_embeddings_fixed.pkl'\n",
    "COMBO_FILE = base_path+'bio-decagon-combo.csv'\n",
    "NUM_EPOCH = 20\n",
    "print(\"\\n[PHASE 3] Running Multi-Label Classification (hierarchical)\")\n",
    "start_time_binary = time.time()\n",
    "    \n",
    "run_multi_label_training(\n",
    "        embeddings_file=EMBEDDINGS_FILE, \n",
    "        combo_file=COMBO_FILE, \n",
    "        epochs=NUM_EPOCHS,\n",
    "        hierarchical=True\n",
    "    )\n",
    "    \n",
    "end_time_binary = time.time()\n",
    "print(f\"\\n✅ PHASE 3 Complete. Time taken: {end_time_binary - start_time_binary:.2f} seconds.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PHASE 3] Running Multi-Label Classification (hierarchical)\n",
      "\n",
      "--- Starting Multi-Label Classification with Cross-Attention (Long-Tailed) ---\n",
      "1. Loading Refined embeddings from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/refined_drug_embeddings_fixed.pkl...\n",
      "   -> Loaded 645 unique drug embeddings.\n",
      "Filtering side effects (>= 500 occurrences)...\n",
      "Original unique side effects: 1317\n",
      "Keeping 963 unique side effects.\n",
      "Filtered dataset size: 4576287 rows.\n",
      "2. Loading hierarchical map from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/bio-decagon-effectcategories.csv...\n",
      "   -> Map built: 561 fine-grained effects mapped to categories.\n",
      "3. Found 63472 unique, embedded drug pairs.\n",
      "   -> Original # of Classes: 963\n",
      "   -> New # of Classes: 592\n",
      "   -> Split Stats: Train=50777 (80%), Val=6347 (10%), Test=6348 (10%)\n",
      "   -> Training on 50777 samples\n",
      "   -> Validating on 6347 samples\n",
      "   -> Testing on 6348 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/2057218711.py:38: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.0259 | Val F1-Micro: 0.1144\n",
      "  --> New best model saved (Val)! F1-Micro: 0.1144\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_hierarchical_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Loss: 0.0240 | Val F1-Micro: 0.1242\n",
      "  --> New best model saved (Val)! F1-Micro: 0.1242\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_hierarchical_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Loss: 0.0235 | Val F1-Micro: 0.1411\n",
      "  --> New best model saved (Val)! F1-Micro: 0.1411\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_hierarchical_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Loss: 0.0232 | Val F1-Micro: 0.1376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Loss: 0.0230 | Val F1-Micro: 0.1242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Loss: 0.0228 | Val F1-Micro: 0.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Loss: 0.0227 | Val F1-Micro: 0.1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Loss: 0.0226 | Val F1-Micro: 0.1326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train Loss: 0.0224 | Val F1-Micro: 0.1275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train Loss: 0.0223 | Val F1-Micro: 0.1138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train Loss: 0.0223 | Val F1-Micro: 0.1630\n",
      "  --> New best model saved (Val)! F1-Micro: 0.1630\n",
      "Checkpoint saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_hierarchical_grover_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train Loss: 0.0222 | Val F1-Micro: 0.1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train Loss: 0.0221 | Val F1-Micro: 0.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train Loss: 0.0220 | Val F1-Micro: 0.1275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train Loss: 0.0220 | Val F1-Micro: 0.1322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train Loss: 0.0219 | Val F1-Micro: 0.1549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train Loss: 0.0218 | Val F1-Micro: 0.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Train Loss: 0.0218 | Val F1-Micro: 0.1369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Train Loss: 0.0217 | Val F1-Micro: 0.1447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39423/3795102580.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Train Loss: 0.0217 | Val F1-Micro: 0.1497\n",
      "\n",
      "--- Final Evaluation on TEST SET (Using Best Validation Model) ---\n",
      "Loaded best model from epoch 10\n",
      "F1 Score (Micro): 0.1632\n",
      "F1 Score (Macro): 0.0194\n",
      "AUROC (Micro):    0.8670\n",
      "AUPR (Micro):     0.4670\n",
      "P@50:             0.4110\n",
      "\n",
      "--- Results saved to /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/experiment_results_grover_hierarchical.csv ---\n",
      "\n",
      "✅ PHASE 3 Complete. Time taken: 545.45 seconds.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "3e80bd29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T10:30:58.325014Z",
     "start_time": "2025-12-18T10:29:12.483817Z"
    }
   },
   "source": [
    "metrics_h = evaluate_saved_model(\n",
    "    checkpoint_path='autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_hierarchical_grover_best_model.pth',\n",
    "    embeddings_file=base_path+'refined_drug_embeddings_fixed.pkl',\n",
    "    combo_file=base_path+'bio-decagon-combo.csv',\n",
    "    hierarchical=True  # Set to True if using hierarchical data\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Saved Model: autodl-tmp/FinalProjectRemote/GROVER_FINAL/multi_label_hierarchical_grover_best_model.pth ---\n",
      "1. Loading Refined embeddings from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/refined_drug_embeddings_fixed.pkl...\n",
      "   -> Loaded 645 unique drug embeddings.\n",
      "Filtering side effects (>= 500 occurrences)...\n",
      "Original unique side effects: 1317\n",
      "Keeping 963 unique side effects.\n",
      "Filtered dataset size: 4576287 rows.\n",
      "2. Loading hierarchical map from /root/autodl-tmp/FinalProjectRemote/GROVER_FINAL/bio-decagon-effectcategories.csv...\n",
      "   -> Map built: 561 fine-grained effects mapped to categories.\n",
      "3. Found 63472 unique, embedded drug pairs.\n",
      "   -> Original # of Classes: 963\n",
      "   -> New # of Classes: 592\n",
      "   -> Split Stats: Train=50777 (80%), Val=6347 (10%), Test=6348 (10%)\n",
      "Loaded weights from epoch 10\n",
      "\n",
      "--- Final Evaluation Metrics (Saved Model) ---\n",
      "F1 Score (Micro): 0.1632\n",
      "F1 Score (Macro): 0.0194\n",
      "AUROC (Micro):    0.8670\n",
      "AUROC (Macro):    0.7936\n",
      "AUPR (Micro):     0.4670\n",
      "AUPR (Macro):     0.2593\n",
      "P@50:             0.4110\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "2566f22a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
